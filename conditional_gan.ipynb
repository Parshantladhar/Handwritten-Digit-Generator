{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parshantladhar/Handwritten-Digit-Generator/blob/main/conditional_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJuY8EcIhIrP"
      },
      "source": [
        "# Conditional GAN\n",
        "\n",
        "**Description:** Training a GAN conditioned on class labels to generate handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIYAjai3hIrQ"
      },
      "source": [
        "Generative Adversarial Networks (GANs) let us generate novel image data, video data,\n",
        "or audio data from a random input. Typically, the random input is sampled\n",
        "from a normal distribution, before going through a series of transformations that turn\n",
        "it into something plausible (image, video, audio, etc.).\n",
        "\n",
        "However, a simple [DCGAN](https://arxiv.org/abs/1511.06434) doesn't let us control\n",
        "the appearance (e.g. class) of the samples we're generating. For instance,\n",
        "with a GAN that generates MNIST handwritten digits, a simple DCGAN wouldn't let us\n",
        "choose the class of digits we're generating.\n",
        "To be able to control what we generate, we need to _condition_ the GAN output\n",
        "on a semantic input, such as the class of an image.\n",
        "\n",
        "In this example, we'll build a **Conditional GAN** that can generate MNIST handwritten\n",
        "digits conditioned on a given class. Such a model can have various useful applications:\n",
        "\n",
        "* let's say you are dealing with an\n",
        "[imbalanced image dataset](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data),\n",
        "and you'd like to gather more examples for the skewed class to balance the dataset.\n",
        "Data collection can be a costly process on its own. You could instead train a Conditional GAN and use\n",
        "it to generate novel images for the class that needs balancing.\n",
        "* Since the generator learns to associate the generated samples with the class labels,\n",
        "its representations can also be used for [other downstream tasks](https://arxiv.org/abs/1809.11096).\n",
        "\n",
        "Following are the references used for developing this example:\n",
        "\n",
        "* [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)\n",
        "* [Lecture on Conditional Generation from Coursera](https://www.coursera.org/lecture/build-basic-generative-adversarial-networks-gans/conditional-generation-inputs-2OPrG)\n",
        "\n",
        "If you need a refresher on GANs, you can refer to the \"Generative adversarial networks\"\n",
        "section of\n",
        "[this resource](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-12/r-3/232).\n",
        "\n",
        "This example requires TensorFlow 2.5 or higher, as well as TensorFlow Docs, which can be\n",
        "installed using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3eJeXoLhIrR",
        "outputId": "ddda54ac-a28a-41b7-a27a-d2dcb1fe914a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqZ3MzfghIrS"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIFPRBHqhIrT"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from tensorflow_docs.vis import embed\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fts0WXBfhIrT"
      },
      "source": [
        "## Constants and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrBbPa9ohIrU"
      },
      "outputs": [],
      "source": [
        "batch_size = 64 # determines how many images are processed together in each training iteration.\n",
        "num_channels = 1 #It indicates that the images used in this model have only one color channel (grayscale).\n",
        "num_classes = 10 # Since we're working with the MNIST dataset, which has 10 digit classes (0 through 9), this variable represents the total number of classes the model needs to learn.\n",
        "image_size = 28 # It specifies the height and width of the images in the MNIST dataset. Each image is 28 pixels by 28 pixels.\n",
        "latent_dim = 128 # This value represents the dimensionality of the latent space, which is a lower-dimensional representation of the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dV0mYerhIrV"
      },
      "source": [
        "## Loading the MNIST dataset and preprocessing it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mmyyd9KhIrV",
        "outputId": "36956bc4-9dde-4531-dfe1-f1a84cbf86c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Shape of training images: (70000, 28, 28, 1)\n",
            "Shape of training labels: (70000, 10)\n"
          ]
        }
      ],
      "source": [
        "# We'll use all the available examples from both the training and test\n",
        "# sets.\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_labels = np.concatenate([y_train, y_test])\n",
        "\n",
        "# Scale the pixel values to [0, 1] range, add a channel dimension to\n",
        "# the images, and one-hot encode the labels.\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0 #(SCALING) converts the image data to floating-point numbers and scales the pixel values to a range of 0 to 1. This is a common preprocessing step for neural networks.\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1)) #(RESHAPING) reshapes the image data to have the format (number of images, height, width, number of channels)\n",
        "all_labels = keras.utils.to_categorical(all_labels, 10) #(ONE-HOT ENCODING) converts the labels (which are originally numbers from 0 to 9) into a one-hot encoding format. This means each label is transformed into a vector of length 10, where only the element corresponding to the digit's value is 1, and the rest are 0s. This representation is more suitable for training classification models.\n",
        "\n",
        "# Create tf.data.Dataset.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels)) #This creates a TensorFlow Dataset object from the NumPy arrays all_digits and all_labels. TensorFlow Datasets are an efficient way to handle large datasets, especially during training\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size) #shuffle(buffer_size=1024): This shuffles the data within a buffer of 1024 elements. Shuffling helps prevent the model from learning patterns based on the order of the data. ##batch(batch_size): This divides the data into batches of size specified by batch_size (which is 64 in this case). Training in batches is more efficient and allows for faster convergence.\n",
        "\n",
        "print(f\"Shape of training images: {all_digits.shape}\")\n",
        "print(f\"Shape of training labels: {all_labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cONCmvqlhIrW"
      },
      "source": [
        "## Calculating the number of input channel for the generator and discriminator\n",
        "\n",
        "In a regular (unconditional) GAN, we start by sampling noise (of some fixed\n",
        "dimension) from a normal distribution. In our case, we also need to account\n",
        "for the class labels. We will have to add the number of classes to\n",
        "the input channels of the generator (noise input) as well as the discriminator\n",
        "(generated image input)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JdHcONGhIrW",
        "outputId": "6e92012e-e12a-4ad5-d039-f1c90b9a6157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "138 11\n"
          ]
        }
      ],
      "source": [
        "generator_in_channels = latent_dim + num_classes #This line calculates the number of input channels for the generator.\n",
        "discriminator_in_channels = num_channels + num_classes #This line calculates the number of input channels for the discriminator.\n",
        "print(generator_in_channels, discriminator_in_channels)\n",
        "\n",
        "# In Summary:\n",
        "\n",
        "# These lines determine how many pieces of information are fed into the generator and discriminator. By including the class label along with the noise (for the generator) and the image (for the discriminator), we allow the model to learn the relationship between images and their classes, enabling it to generate images based on specific conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OExQCc10hIrW"
      },
      "source": [
        "## Creating the discriminator and generator\n",
        "\n",
        "The model definitions (`discriminator`, `generator`, and `ConditionalGAN`) have been\n",
        "adapted from [this example](https://keras.io/guides/customizing_what_happens_in_fit/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4jYchvDhIrW"
      },
      "outputs": [],
      "source": [
        "# Create the discriminator.\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),# 64 and 128 specify the number of filters in each layer. ##(3, 3) defines the size of the filters (3x3 pixels). ###strides=(2, 2) means the filter moves 2 pixels at a time, effectively downsampling the image. ####padding=\"same\" ensures the output has the same dimensions as the input.\n",
        "        layers.LeakyReLU(negative_slope=0.2),#These are activation functions. They introduce non-linearity to the model, allowing it to learn complex patterns. LeakyReLU is a variation of ReLU (Rectified Linear Unit)  that allows a small, non-zero gradient for negative inputs.\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.GlobalMaxPooling2D(),#This layer reduces the spatial dimensions of the feature maps by taking the maximum value within each feature map.\n",
        "        layers.Dense(1),# A fully connected layer with a single output neuron. This output represents the discriminator's prediction (whether the input is real or fake).\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "#Discriminator Summary\n",
        "# In essence, the discriminator is a convolutional neural network that takes an image (and its label information) as input and outputs a single value indicating whether the image is real or fake.\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer((generator_in_channels,)),\n",
        "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
        "        # 7x7x(128 + num_classes) map.\n",
        "        layers.Dense(7 * 7 * generator_in_channels), # A fully connected layer that expands the input vector into a higher-dimensional representation.\n",
        "        layers.LeakyReLU(negative_slope=0.2), #Same activation function as in the discriminator.\n",
        "        layers.Reshape((7, 7, generator_in_channels)), # Reshapes the output of the previous layer into a 3D tensor to be used by the following convolutional layers.\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"), #These are transposed convolutional layers. They effectively upsample the input, gradually increasing its spatial dimensions to generate an image. The parameters are similar to Conv2D but work in reverse.\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"), #The final convolutional layer produces the output image. ##1 indicates a single output channel (grayscale). ###activation=\"sigmoid\" ensures the pixel values are in the range [0, 1].\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "# The generator takes a random noise vector (and a class label) as input and transforms it into a synthetic image that ideally resembles a real image from the target dataset.\n",
        "\n",
        "# In summary, these two code blocks define the architectures of the discriminator and generator networks within the Conditional GAN. They work together in an adversarial process, where the generator tries to create realistic images, and the discriminator tries to distinguish between real and generated images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa1RgCbnhIrX"
      },
      "source": [
        "## Creating a `ConditionalGAN` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF8gKw_phIrX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ConditionalGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):#(d_optimizer) The optimizer used to update the discriminator's weights. ##(g_optimizer) The optimizer used to update the generator's weights. ###loss_fn: The loss function used to measure how well the networks are performing.\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        real_images, one_hot_labels = data\n",
        "\n",
        "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
        "        # the images. This is for the discriminator.\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        image_one_hot_labels = ops.repeat(\n",
        "            image_one_hot_labels, repeats=[image_size * image_size]\n",
        "        )\n",
        "        image_one_hot_labels = ops.reshape(\n",
        "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space and concatenate the labels.\n",
        "        # This is for the generator.\n",
        "        batch_size = ops.shape(real_images)[0]\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
        "        )\n",
        "        random_vector_labels = ops.concatenate(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Decode the noise (guided by labels) to fake images.\n",
        "        generated_images = self.generator(random_vector_labels)\n",
        "\n",
        "        # Combine them with real images. Note that we are concatenating the labels\n",
        "        # with these images here.\n",
        "        fake_image_and_labels = ops.concatenate(\n",
        "            [generated_images, image_one_hot_labels], -1\n",
        "        )\n",
        "        real_image_and_labels = ops.concatenate([real_images, image_one_hot_labels], -1)\n",
        "        combined_images = ops.concatenate(\n",
        "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
        "        )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images.\n",
        "        labels = ops.concatenate(\n",
        "            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "\n",
        "        # Train the discriminator.\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space.\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
        "        )\n",
        "        random_vector_labels = ops.concatenate(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Assemble labels that say \"all real images\".\n",
        "        misleading_labels = ops.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_vector_labels)\n",
        "            fake_image_and_labels = ops.concatenate(\n",
        "                [fake_images, image_one_hot_labels], -1\n",
        "            )\n",
        "            predictions = self.discriminator(fake_image_and_labels)\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWlCx7l8hIrY"
      },
      "source": [
        "## Training the Conditional GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "FoofgeofhIrY",
        "outputId": "e826f201-6f33-4cff-9ec3-175708b9e3e7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - d_loss: 0.4254 - g_loss: 1.6097\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-07b09c2d7735>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcond_gan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cond_gan = ConditionalGAN(\n",
        "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
        ")\n",
        "cond_gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "cond_gan.fit(dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMyzA-BghIrY"
      },
      "source": [
        "## Interpolating between classes with the trained generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISCe1458hIrY"
      },
      "outputs": [],
      "source": [
        "# We first extract the trained generator from our Conditional GAN.\n",
        "trained_gen = cond_gan.generator\n",
        "\n",
        "# Choose the number of intermediate images that would be generated in\n",
        "# between the interpolation + 2 (start and last images).\n",
        "num_interpolation = 9  # @param {type:\"integer\"}\n",
        "\n",
        "# Sample noise for the interpolation.\n",
        "interpolation_noise = keras.random.normal(shape=(1, latent_dim))\n",
        "interpolation_noise = ops.repeat(interpolation_noise, repeats=num_interpolation)\n",
        "interpolation_noise = ops.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
        "\n",
        "\n",
        "def interpolate_class(first_number, second_number):\n",
        "    # Convert the start and end labels to one-hot encoded vectors.\n",
        "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
        "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
        "    first_label = ops.cast(first_label, \"float32\")\n",
        "    second_label = ops.cast(second_label, \"float32\")\n",
        "\n",
        "    # Calculate the interpolation vector between the two labels.\n",
        "    percent_second_label = ops.linspace(0, 1, num_interpolation)[:, None]\n",
        "    percent_second_label = ops.cast(percent_second_label, \"float32\")\n",
        "    interpolation_labels = (\n",
        "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
        "    )\n",
        "\n",
        "    # Combine the noise and the labels and run inference with the generator.\n",
        "    noise_and_labels = ops.concatenate([interpolation_noise, interpolation_labels], 1)\n",
        "    fake = trained_gen.predict(noise_and_labels)\n",
        "    return fake\n",
        "\n",
        "\n",
        "start_class = 2  # @param {type:\"slider\", min:0, max:9, step:1}\n",
        "end_class = 6  # @param {type:\"slider\", min:0, max:9, step:1}\n",
        "\n",
        "fake_images = interpolate_class(start_class, end_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpRTxgS-hIrY"
      },
      "source": [
        "Here, we first sample noise from a normal distribution and then we repeat that for\n",
        "`num_interpolation` times and reshape the result accordingly.\n",
        "We then distribute it uniformly for `num_interpolation`\n",
        "with the label identities being present in some proportion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlV9dGbChIrY"
      },
      "outputs": [],
      "source": [
        "fake_images *= 255.0\n",
        "converted_images = fake_images.astype(np.uint8)\n",
        "converted_images = ops.image.resize(converted_images, (96, 96)).numpy().astype(np.uint8)\n",
        "imageio.mimsave(\"animation.gif\", converted_images[:, :, :, 0], fps=1)\n",
        "embed.embed_file(\"animation.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eva82IohIrY"
      },
      "source": [
        "We can further improve the performance of this model with recipes like\n",
        "[WGAN-GP](https://keras.io/examples/generative/wgan_gp).\n",
        "Conditional generation is also widely used in many modern image generation architectures like\n",
        "[VQ-GANs](https://arxiv.org/abs/2012.09841), [DALL-E](https://openai.com/blog/dall-e/),\n",
        "etc.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/conditional-gan) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/conditional-GAN)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}